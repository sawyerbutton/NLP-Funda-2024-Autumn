{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 在 GPU 上使用DeepSpeed-Inference加速GPT的Inference过程\n",
    "\n",
    "使用Hugging Face Transformers和DeepSpeed-Inference来优化GPT-2/GPT-J的推理性能。\n",
    "\n",
    "- 设置开发环境\n",
    "- 加载初始的GPT-J模型并设定基线\n",
    "- 使用DeepSpeed的InferenceEngine优化GPT-J以适配GPU\n",
    "- 评估性能和速度\n",
    "\n",
    "注意！本教程是在包含NVIDIA T4的g4dn.2xlarge AWS EC2实例上创建和运行的，请自行学习如何使用云服务器 或 云服务器大模型服务提供商。\n",
    "\n",
    "---\n",
    "\n",
    "## 什么是Deepspeed Inference\n",
    "\n",
    "- DeepSpeed-Inference是DeepSpeed框架的扩展，用于提升推理工作负载。\n",
    "- DeepSpeed Inference结合了张量并行（tensor parallelism）、流水线并行（pipeline-parallelism）等模型并行技术，并使用了自定义优化的CUDA内核。\n",
    "- DeepSpeed为使用DeepSpeed、Megatron和HuggingFace训练的兼容Transformer模型提供了无缝的推理模式。\n",
    "- 举例来说，DeepSpeed-Inference集成了模型并行技术，允许您在多GPU上运行大型语言模型（LLM）的推理，如具有1760亿参数的BLOOM。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 设置开发环境\n",
    "\n",
    "- 安装DeepSpeed，以及PyTorch、Transformers和其他一些库。\n",
    "- 运行以下代码单元格将安装所有必需的包。\n",
    "\n",
    "_注意：需要一台带有GPU并安装了兼容的CUDA的机器。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch==1.11.0 torchvision==0.12.0 --extra-index-url https://download.pytorch.org/whl/cu113 --upgrade -q \n",
    "# !pip install deepspeed==0.7.2 --upgrade -q \n",
    "!pip install git+https://github.com/microsoft/DeepSpeed.git@ds-inference/support-large-token-length --upgrade\n",
    "!pip install transformers[sentencepiece]==4.21.2 accelerate --upgrade -q \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 在开始之前，我们需要确认所有的packages都正常安装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re  # 导入正则表达式库re，用于字符串匹配\n",
    "import torch  # 导入PyTorch库\n",
    "\n",
    "# 检查DeepSpeed的安装情况\n",
    "# 使用deepspeed.env_report命令输出当前DeepSpeed环境的信息\n",
    "report = !python3 -m deepspeed.env_report\n",
    "\n",
    "# 使用正则表达式编译一个模式，用于匹配输出中的'ninja'状态是否为'OKAY'\n",
    "r = re.compile('.*ninja.*OKAY.*')\n",
    "\n",
    "# 断言判断，如果report中没有匹配到'ninja' OKAY状态，则抛出异常提示DeepSpeed Inference未正确安装\n",
    "assert any(r.match(line) for line in report) == True, \"DeepSpeed Inference not correctly installed\"\n",
    "\n",
    "# 检查CUDA和PyTorch版本\n",
    "# 从torch.__version__获取torch和cuda版本信息\n",
    "torch_version, cuda_version = torch.__version__.split(\"+\")\n",
    "\n",
    "# 只保留torch版本的前两位，比如从'1.9.1'中提取'1.9'\n",
    "torch_version = \".\".join(torch_version.split(\".\")[:2])\n",
    "\n",
    "# 格式化CUDA版本为标准显示格式，例如'cu101'变为'10.1'\n",
    "cuda_version = f\"{cuda_version[2:4]}.{cuda_version[4:]}\"\n",
    "\n",
    "# 正则表达式用于匹配DeepSpeed报告中的torch版本信息\n",
    "r = re.compile(f'.*torch.*{torch_version}.*')\n",
    "# 如果版本不匹配，抛出错误提示\n",
    "assert any(r.match(line) for line in report) == True, \"Wrong Torch version\"\n",
    "\n",
    "# 正则表达式用于匹配DeepSpeed报告中的cuda版本信息\n",
    "r = re.compile(f'.*cuda.*{cuda_version}.*')\n",
    "# 如果版本不匹配，抛出错误提示\n",
    "assert any(r.match(line) for line in report) == True, \"Wrong Cuda version\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 加载原生 GPT-J 模型并设置baseline\n",
    "\n",
    "- 在设置好环境后，为模型创建一个基线\n",
    "- 此处使用的是EleutherAI/gpt-j-6B，一个由EleutherAI训练的GPT-J 6B模型。该模型在一个大规模的精选数据集The Pile上进行了训练\n",
    "- 训练过程中，使用TPU v3-256 pod在383,500步内处理了4020亿个tokens\n",
    "- 它作为一个自回归语言模型（autoregressive language model）进行训练\n",
    "- 使用交叉熵损失（cross-entropy loss）来最大化正确预测下一个token的概率\n",
    "\n",
    "- 使用transformers加载模型并运行推理，创建基线。\n",
    "\n",
    "_注意：这里创建了一个单独的仓库，其中包含分片的fp16权重，以便通过使用device_map功能自动将分片的检查点加载到GPU上，从而更容易在较小的CPU上加载模型_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model is loaded on device cuda\n"
     ]
    }
   ],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline  # 导入Transformers库中的必要模块\n",
    "\n",
    "# Hugging Face模型库中的模型仓库ID\n",
    "model_id = \"philschmid/gpt-j-6B-fp16-sharded\"\n",
    "\n",
    "# 加载模型和分词器\n",
    "# 使用AutoTokenizer加载分词器，该分词器与模型ID匹配\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 使用AutoModelForCausalLM加载因果语言模型\n",
    "# 这里设置torch_dtype为float16以减少内存使用，同时使用device_map=\"auto\"自动将所有模型分片放置到GPU上\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# 输出确认模型已加载到设备上\n",
    "print(f\"model is loaded on device {model.device.type}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input payload: \n",
      " \n",
      "Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\n",
      "prediction: \n",
      " \n",
      " 's Friday evening for the British and you can feel that coming in on top of a Friday, please try to spend a quiet time tonight. Thankyou, Philipp\n",
      "\n",
      "Annette\n",
      "\n",
      "Thank you for your reply to my last email. Regarding your issue with your new credit card please forward your request by email to \"customer.service@lodging.com\" In order for this to happen the email you send will need to include your full name, card number and the account number that your new card is linked to. Your credit card account number should be at the top of the email to avoid any misinterpretation of the request\n"
     ]
    }
   ],
   "source": [
    "# 定义输入文本（payload），这是需要模型生成回复的内容\n",
    "payload = (\"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. \"\n",
    "           \"What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this \"\n",
    "           \"email in the next 7 days. Best regards and have a nice weekend but it\")\n",
    "\n",
    "# 使用分词器将输入文本转换为模型可接受的输入ID，并将其放置到与模型相同的设备上（如GPU）\n",
    "input_ids = tokenizer(payload, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# 打印输入的payload内容\n",
    "print(f\"input payload: \\n\\n{payload}\")\n",
    "\n",
    "# 使用加载的模型进行文本生成推理\n",
    "# 参数解释：\n",
    "# - `do_sample=True`: 表示使用采样方法生成文本（而不是贪婪搜索）\n",
    "# - `num_beams=1`: 表示使用单束搜索（即不进行束搜索优化）\n",
    "# - `min_length=128`: 生成的最小长度为128个token\n",
    "# - `max_new_tokens=128`: 最大生成128个新的token\n",
    "logits = model.generate(input_ids, do_sample=True, num_beams=1, min_length=128, max_new_tokens=128)\n",
    "\n",
    "# 打印模型生成的预测输出\n",
    "# 使用`tokenizer.decode`方法将生成的token ID转回人类可读的文本\n",
    "# `logits[0].tolist()[len(input_ids[0]):]`用于提取生成的部分，而不是包括输入部分\n",
    "print(f\"prediction: \\n\\n{tokenizer.decode(logits[0].tolist()[len(input_ids[0]):])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"My name is Philipp and I'm from Germany.\\nI have been a collector of music since I am a small child. I own about 4500 vinyls and 10k CDs but my collection is by no means the most important thing in my life. I am married with two kids.\\nI got into the IT business many years ago, worked in the game industry where I learned a lot about programming, but now I live on the other side of the fence. We sell our house and are\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试模型生成能力\n",
    "# 定义一个简单的输入示例\n",
    "example = \"My name is Philipp and I\"\n",
    "\n",
    "# 使用分词器将输入示例转换为模型可接受的输入ID，并将其转移到模型所在的设备（GPU）\n",
    "input_ids = tokenizer(example, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# 使用加载的模型生成文本\n",
    "# 参数解释：\n",
    "# - `do_sample=True`: 表示使用采样策略来生成输出，增加多样性\n",
    "# - `max_length=100`: 生成的文本最大长度为100个token\n",
    "logits = model.generate(input_ids, do_sample=True, max_length=100)\n",
    "\n",
    "# 将生成的token ID转换为人类可读的文本\n",
    "output_text = tokenizer.decode(logits[0].tolist())\n",
    "\n",
    "# 打印生成的文本结果\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用measure_latency函数来创建延迟基线，该函数通过一个简单的Python循环来运行推理，并计算模型的平均延迟（avg）、中位延迟（mean）和95百分位延迟（p95）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import perf_counter  # 从time库中导入perf_counter，用于高精度计时\n",
    "import numpy as np  # 导入numpy库，用于计算延迟的统计量\n",
    "import transformers  # 导入transformers库\n",
    "\n",
    "# 隐藏生成时的警告信息\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "def measure_latency(model, tokenizer, payload, generation_args={}, device=model.device):\n",
    "    \"\"\"\n",
    "    测量模型的推理延迟。\n",
    "\n",
    "    参数：\n",
    "    - model: 已加载的语言模型\n",
    "    - tokenizer: 模型对应的分词器\n",
    "    - payload: 要进行推理的输入文本\n",
    "    - generation_args: 生成参数的字典（默认空）\n",
    "    - device: 运行推理的设备（默认为模型的设备）\n",
    "\n",
    "    返回值：\n",
    "    - 延迟统计信息的字符串格式\n",
    "    - p95延迟值\n",
    "    \"\"\"\n",
    "    # 使用分词器将输入文本转换为模型可接受的输入ID，并将其放置到指定设备上\n",
    "    input_ids = tokenizer(payload, return_tensors=\"pt\").input_ids.to(device)\n",
    "    latencies = []  # 初始化一个列表来存储每次推理的延迟时间\n",
    "\n",
    "    # 预热模型（warm up）\n",
    "    for _ in range(2):\n",
    "        _ = model.generate(input_ids, **generation_args)\n",
    "\n",
    "    # 测量推理延迟\n",
    "    for _ in range(10):\n",
    "        start_time = perf_counter()  # 记录推理开始时间\n",
    "        _ = model.generate(input_ids, **generation_args)  # 运行模型推理\n",
    "        latency = perf_counter() - start_time  # 计算单次推理的延迟\n",
    "        latencies.append(latency)  # 将延迟添加到列表中\n",
    "\n",
    "    # 计算延迟的统计量\n",
    "    time_avg_ms = 1000 * np.mean(latencies)  # 计算平均延迟（毫秒）\n",
    "    time_std_ms = 1000 * np.std(latencies)  # 计算延迟的标准差（毫秒）\n",
    "    time_p95_ms = 1000 * np.percentile(latencies, 95)  # 计算95百分位延迟（毫秒）\n",
    "\n",
    "    # 返回格式化的延迟统计信息和95百分位延迟值\n",
    "    return f\"P95 latency (ms) - {time_p95_ms:.2f}; Average latency (ms) - {time_avg_ms:.2f} ± {time_std_ms:.2f};\", time_p95_ms\n",
    "\n",
    "# 示例使用\n",
    "payload = \"My name is Philipp and I\"\n",
    "generation_args = {'do_sample': True, 'max_length': 100}\n",
    "\n",
    "# 调用measure_latency函数并输出结果\n",
    "latency_info, p95_latency = measure_latency(model, tokenizer, payload, generation_args)\n",
    "print(latency_info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码策略与生成设置\n",
    "使用贪婪搜索（greedy search）作为解码策略，并将生成128个新的token，输入的长度也为128个token。\n",
    "\n",
    "贪婪搜索（Greedy Search）：\n",
    "\n",
    "贪婪搜索是一种常见的解码策略，在生成文本时，每一步选择概率最高的下一个token。虽然这种方法能够快速生成文本，但容易陷入局部最优解，可能导致生成的文本缺乏多样性。\n",
    "与其他解码策略（如束搜索、采样方法）相比，贪婪搜索通常更快，但可能不如其他方法生成的文本质量高，尤其是在需要生成更复杂或更具创意的内容时。\n",
    "生成设置：\n",
    "在推理过程中，将模型的输入设置为128个token，这意味着输入文本将被分割为128个token的序列。\n",
    "然后，模型将基于输入生成新的文本，生成的输出长度也为128个token。目的是确保生成的输出具有足够的上下文，以便进行有效的性能评估。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length is: 128\n",
      "Vanilla model: P95 latency (ms) - 8985.898722249989; Average latency (ms) - 8955.07 +\\- 24.34;\n"
     ]
    }
   ],
   "source": [
    "# 扩展输入文本（payload），通过重复两次相同的内容来增加输入序列的长度\n",
    "payload = (\n",
    "    \"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. \"\n",
    "    \"What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer \"\n",
    "    \"this email in the next 7 days. Best regards and have a nice weekend but it\"\n",
    ") * 2  # 将输入文本重复两次以扩展长度\n",
    "\n",
    "# 打印扩展后的输入序列长度\n",
    "print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "# 生成的参数设置\n",
    "generation_args = dict(\n",
    "    do_sample=False,  # 不使用采样，使用贪婪搜索\n",
    "    num_beams=1,      # 使用单束搜索（不进行束搜索优化）\n",
    "    min_length=128,   # 生成的最小长度为128个token\n",
    "    max_new_tokens=128  # 最大生成128个新的token\n",
    ")\n",
    "\n",
    "# 使用测量延迟函数来评估未优化模型（Vanilla model）的延迟\n",
    "vanilla_results = measure_latency(model, tokenizer, payload, generation_args)\n",
    "\n",
    "# 打印未优化模型的延迟结果\n",
    "print(f\"Vanilla model: {vanilla_results[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "模型在生成128个token的情况下，达到了8.9秒的推理延迟，相当于每个token的生成时间为69毫秒"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 使用 DeepSpeeds 的 `InferenceEngine` 优化 GPT-J\n",
    "\n",
    "- 接下来也是最重要的一步是优化模型以在GPU上进行推理。\n",
    "- 使用DeepSpeed的InferenceEngine来实现。\n",
    "- InferenceEngine通过init_inference方法进行初始化。init_inference方法至少需要以下几个参数：\n",
    "\n",
    "- model: 需要优化的模型。\n",
    "- mp_size: 使用的GPU数量（模型并行的数量）。\n",
    "- dtype: 使用的数据类型（如float16）。\n",
    "- replace_with_kernel_inject: 是否注入自定义CUDA内核。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch  # 导入PyTorch库\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM  # 导入Transformers库中的模型和分词器加载方法\n",
    "import deepspeed  # 导入DeepSpeed库\n",
    "\n",
    "# Hugging Face模型库中的模型仓库ID\n",
    "model_id = \"philschmid/gpt-j-6B-fp16-sharded\"\n",
    "\n",
    "# 加载模型和分词器\n",
    "# 使用AutoTokenizer加载分词器\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "# 使用AutoModelForCausalLM加载因果语言模型，设置权重数据类型为float16\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n",
    "\n",
    "# 初始化DeepSpeed推理引擎\n",
    "# 参数解释：\n",
    "# - model: 要优化的Transformer模型实例\n",
    "# - mp_size: 使用的GPU数量（这里设置为1）\n",
    "# - dtype: 模型权重的数据类型（这里设置为float16）\n",
    "# - replace_method: 设置为\"auto\"，让DeepSpeed自动识别需要替换的层\n",
    "# - replace_with_kernel_inject: 设置为True，使用DeepSpeed的内核注入器替换默认的CUDA内核\n",
    "ds_model = deepspeed.init_inference(\n",
    "    model=model,  # 需要优化的模型\n",
    "    mp_size=1,  # 使用1个GPU\n",
    "    dtype=torch.float16,  # 使用半精度浮点数（fp16）以减少内存占用\n",
    "    replace_method=\"auto\",  # 让DeepSpeed自动识别和替换需要优化的层\n",
    "    replace_with_kernel_inject=True  # 使用DeepSpeed的内核注入器进行优化\n",
    ")\n",
    "\n",
    "# 打印确认模型已加载到哪个设备上\n",
    "print(f\"model is loaded on device {ds_model.module.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在检查模型的计算图，验证原始的GPTJLayer已经被HFGPTJLayer替换，而HFGPTJLayer包含了DeepSpeedTransformerInference模块。\n",
    "\n",
    "```python\n",
    "InferenceEngine(\n",
    "  (module): GPTJForCausalLM(  # GPT-J的因果语言模型模块\n",
    "    (transformer): GPTJModel(  # GPT-J的Transformer模型\n",
    "      (wte): Embedding(50400, 4096)  # 词嵌入层（Embedding layer）\n",
    "      (drop): Dropout(p=0.0, inplace=False)  # Dropout层，丢弃概率为0.0\n",
    "      (h): ModuleList(  # 模型的主体部分是一个模块列表（包括多个Transformer层）\n",
    "        (0): DeepSpeedTransformerInference(  # 使用DeepSpeed优化的Transformer推理模块\n",
    "          (attention): DeepSpeedSelfAttention()  # 使用DeepSpeed优化的自注意力层\n",
    "          (mlp): DeepSpeedMLP()  # 使用DeepSpeed优化的多层感知机（MLP）层\n",
    "        )\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepspeed.ops.transformer.inference import DeepSpeedTransformerInference  # 导入DeepSpeed的推理优化模块\n",
    "\n",
    "# 断言检查：验证模型的第一个Transformer层是否是DeepSpeed优化的Transformer推理模块\n",
    "assert isinstance(ds_model.module.transformer.h[0], DeepSpeedTransformerInference) == True, \"Model not successfully initialized\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My name is Philipp and I live in Freiburg in Germany and I have a project called Cenapen. After three months in development already it is finally finished – and it is a Linux based device / operating system on an ARM Cortex A9 processor on a Raspberry Pi.\\n\\nAt the moment it offers the possibility to store data locally, it can retrieve data from a local, networked or web based Sqlite database (I’m writing this tutorial while I’'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 测试模型生成能力\n",
    "# 定义一个简单的输入示例\n",
    "example = \"My name is Philipp and I\"\n",
    "\n",
    "# 使用分词器将输入示例转换为模型可接受的输入ID，并将其转移到模型所在的设备（GPU）\n",
    "input_ids = tokenizer(example, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# 使用DeepSpeed优化后的模型生成文本\n",
    "# 参数解释：\n",
    "# - `do_sample=True`: 表示使用采样策略来生成输出，增加多样性\n",
    "# - `max_length=100`: 生成的文本最大长度为100个token\n",
    "logits = ds_model.generate(input_ids, do_sample=True, max_length=100)\n",
    "\n",
    "# 将生成的token ID转换为人类可读的文本\n",
    "output_text = tokenizer.decode(logits[0].tolist())\n",
    "\n",
    "# 打印生成的文本结果\n",
    "print(output_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 评价效率和速度\n",
    "\n",
    "作为最后一步，需要详细分析优化后的模型性能。应用优化技术（如图优化和混合精度）不仅会影响性能（延迟），还可能对模型的准确性产生影响。因此，加速模型往往伴随着一定的权衡。\n",
    "\n",
    "- 性能与准确性的权衡：\n",
    "\n",
    "- 通过图优化（graph optimizations）和混合精度（mixed-precision）等技术，可以显著提高模型推理的速度和降低延迟。\n",
    "但是，这些技术也可能导致模型的准确性有所下降。对于实际应用，优化的目标需要在性能和准确性之间找到合适的平衡点。\n",
    "\n",
    "- 测试优化后的模型性能：\n",
    "\n",
    "- 使用和原始模型（vanilla model）相同的生成参数（generation_args）来测试优化后的模型性能。这确保了测试的公平性，可以直接对比优化前后的性能差异。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Payload sequence length is: 128\n",
      "DeepSpeed model: P95 latency (ms) - 6577.044982599967; Average latency (ms) - 6569.11 +\\- 6.57;\n"
     ]
    }
   ],
   "source": [
    "# 扩展输入文本（payload），通过重复两次相同的内容来增加输入序列的长度\n",
    "payload = (\n",
    "    \"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. \"\n",
    "    \"What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer \"\n",
    "    \"this email in the next 7 days. Best regards and have a nice weekend but it\"\n",
    "    * 2  # 将输入文本重复两次以扩展长度\n",
    ")\n",
    "\n",
    "# 打印扩展后的输入序列长度\n",
    "print(f'Payload sequence length is: {len(tokenizer(payload)[\"input_ids\"])}')\n",
    "\n",
    "# 生成参数设置\n",
    "generation_args = dict(\n",
    "    do_sample=False,  # 不使用采样，使用贪婪搜索\n",
    "    num_beams=1,      # 使用单束搜索（不进行束搜索优化）\n",
    "    min_length=128,   # 生成的最小长度为128个token\n",
    "    max_new_tokens=128  # 最大生成128个新的token\n",
    ")\n",
    "\n",
    "# 使用之前定义的measure_latency函数来测试DeepSpeed优化后的模型的推理延迟\n",
    "# 参数包括：DeepSpeed优化后的模型、分词器、扩展后的输入文本、生成参数以及模型所在的设备\n",
    "ds_results = measure_latency(ds_model, tokenizer, payload, generation_args, ds_model.module.device)\n",
    "\n",
    "# 打印DeepSpeed优化后的模型的延迟结果\n",
    "print(f\"DeepSpeed model: {ds_results[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input payload: \n",
      " \n",
      "Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but it\n",
      "prediction: \n",
      " \n",
      " 's not over yet.\n",
      "\n",
      "I am getting in touch with you because i didn't get a response from you. What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer this email in the next 7 days. Best regards and have a nice weekend but\n"
     ]
    }
   ],
   "source": [
    "# 定义输入文本（payload）\n",
    "payload = (\n",
    "    \"Hello my name is Philipp. I am getting in touch with you because i didn't get a response from you. \"\n",
    "    \"What do I need to do to get my new card which I have requested 2 weeks ago? Please help me and answer \"\n",
    "    \"this email in the next 7 days. Best regards and have a nice weekend but it\"\n",
    ")\n",
    "\n",
    "# 使用分词器将输入文本转换为模型可接受的输入ID，并将其放置到与模型相同的设备（GPU）上\n",
    "input_ids = tokenizer(payload, return_tensors=\"pt\").input_ids.to(model.device)\n",
    "\n",
    "# 打印输入的payload内容\n",
    "print(f\"input payload: \\n\\n{payload}\")\n",
    "\n",
    "# 使用DeepSpeed优化后的模型生成文本\n",
    "# 参数解释：\n",
    "# - `do_sample=False`: 不使用采样，使用确定性的方法生成文本\n",
    "# - `num_beams=2`: 使用束宽为2的束搜索（Beam Search）来生成输出\n",
    "# - `min_length=64`: 生成的最小长度为64个token\n",
    "# - `max_new_tokens=64`: 最大生成64个新的token\n",
    "logits = ds_model.generate(input_ids, do_sample=False, num_beams=2, min_length=64, max_new_tokens=64)\n",
    "\n",
    "# 将生成的token ID转换为人类可读的文本\n",
    "output_text = tokenizer.decode(logits[0].tolist()[len(input_ids[0]):])\n",
    "\n",
    "# 打印模型生成的预测输出\n",
    "print(f\"prediction: \\n\\n{output_text}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化后的DeepSpeed模型在生成128个token时达到了6.5秒的推理延迟，相当于每个token的生成时间为50毫秒。\n",
    "\n",
    "性能提升分析\n",
    "性能改进结果：\n",
    "\n",
    "优化前的GPT-J-6B模型生成128个token的延迟为8.9秒（即69毫秒/token）。\n",
    "优化后的模型生成128个token的延迟降低到6.5秒（即50毫秒/token）。\n",
    "提升幅度计算：\n",
    "\n",
    "优化前的每个token延迟为69毫秒，优化后为50毫秒。\n",
    "计算性能提升倍数：\n",
    "提升倍数1.38\n",
    "结果表明，经过优化后，模型的推理速度提升了1.38倍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('dev')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f6dd96c16031089903d5a31ec148b80aeb0d39c32affb1a1080393235fbfa2fc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
